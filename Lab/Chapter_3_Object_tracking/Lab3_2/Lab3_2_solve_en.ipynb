{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Traffic sign detection\n",
    "\n",
    "Welcome to Lab 3.2. In this lab, we will learn how to use a pre-trained YOLO model for traffic sign detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instructions\n",
    "\n",
    "Below are detailed instructions to help you program and run the traffic sign tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "# Ensure proper path handling for Windows\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# Handle library conflicts\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDetector():\n",
    "    def __init__(self, model_path, force_reload):\n",
    "        # Initialize the SignDetector object\n",
    "        self.model = self.load_model(model_path, force_reload)\n",
    "        # Print the list of class names\n",
    "        self.classes = self.model.names\n",
    "        print(self.classes)\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_path, force_reload):\n",
    "        # Load the model from the model_path or a pre-trained model\n",
    "        if model_path:\n",
    "            # Load a custom model from the model_path\n",
    "            model = torch.hub.load(\n",
    "                'ultralytics/yolov5',\n",
    "                'custom',\n",
    "                path=model_path,\n",
    "                force_reload=force_reload\n",
    "            )\n",
    "        else:\n",
    "            # Load the pre-trained yolov5s model\n",
    "            model = torch.hub.load(\n",
    "                'ultralytics/yolov5',\n",
    "                'yolov5s',\n",
    "                pretrained=True\n",
    "            )\n",
    "\n",
    "        return model  # Return the loaded model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame_rgb = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        # Predict objects in the frame\n",
    "        results = self.model(frame_rgb)\n",
    "        # Extract labels and coordinates of detected objects\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "\n",
    "        return labels, cord  # Return labels and coordinates\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        # Convert class number to class label\n",
    "        return self.classes[int(x)]  # Return the corresponding class label\n",
    "\n",
    "    def plot_boxes(self, results, height, width, confidence=0.3):\n",
    "        # Draw bounding boxes around detected objects\n",
    "        # Extract labels and coordinates from the results\n",
    "        labels, cord = results\n",
    "        # List to store detections\n",
    "        detections = []\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            # Coordinates of the object\n",
    "            row = cord[i]\n",
    "            # Check the confidence of the object\n",
    "            if row[4] >= confidence:\n",
    "                # Calculate the bounding box coordinates\n",
    "                x1, y1, x2, y2 = int(\n",
    "                    row[0]*width), int(row[1]*height), int(row[2]*width), int(row[3]*height)\n",
    "\n",
    "                # Confidence of the detection\n",
    "                conf = float(row[4].item())\n",
    "                # Label of the object\n",
    "                class_label = self.class_to_label(\n",
    "                    labels[i])\n",
    "                # Add the object to the list of detections\n",
    "                detections.append(\n",
    "                    ([x1, y1, int(x2-x1), int(y2-y1)], conf, class_label))\n",
    "\n",
    "        return detections  # Return the list of detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implement the YOLOv5 Pre-trained Model and DeepSORT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "ip_lap_lead = \"192.168.109.105\"\n",
    "port_lap_lead = 7001\n",
    "\n",
    "ip_lap = \"192.168.109.105\"\n",
    "port_lap = 3000\n",
    "\n",
    "# Define the path to the YOLO model weights for traffic sign detection\n",
    "model_path = r'..\\Other\\YoloWeights\\TrafficSign.pt'\n",
    "\n",
    "# Initialize the traffic sign detector with the specified model path\n",
    "traffic_signs_detector = SignDetector(model_path=model_path, force_reload=True)\n",
    "\n",
    "# Initialize the object tracker with specified parameters\n",
    "object_tracker = DeepSort(\n",
    "    max_age=3, n_init=2, nms_max_overlap=1.0, max_cosine_distance=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "BUFFER_SIZE = 65355\n",
    "\n",
    "\n",
    "def create_udp_socket_listen(ip, port):\n",
    "    \"\"\"Create and bind a UDP socket for listening\"\"\"\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.bind((ip, port))\n",
    "    return sock\n",
    "\n",
    "\n",
    "def create_udp_socket_send():\n",
    "    \"\"\"Create and configure a UDP socket for sending\"\"\"\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, BUFFER_SIZE)\n",
    "    return sock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "END_MARKER = b'\\xff\\xd9'\n",
    "IMAGE_RESIZE_DIMS = (1280, 720)\n",
    "queue_image = Queue()\n",
    "MAX_QUEUE_SIZE = 3\n",
    "\n",
    "\n",
    "def get_image_from_cam():\n",
    "    \"\"\"Receive images via UDP and put them in the queue.\"\"\"\n",
    "    global stop_event\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    while cap.isOpened() and (not stop_event.is_set()):\n",
    "        _, image = cap.read()\n",
    "        queue_image.put(image)\n",
    "\n",
    "        if queue_image.qsize() >= MAX_QUEUE_SIZE:\n",
    "            queue_image.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_leader():\n",
    "    \"\"\"Receive images via UDP and put them in the queue.\"\"\"\n",
    "    global stop_event\n",
    "\n",
    "    sock = create_udp_socket_listen(ip_lap, port_lap)\n",
    "    print(f\"Listening on IP {ip_lap}, UDP port {port_lap}\")\n",
    "\n",
    "    image_data = bytearray()\n",
    "    while not stop_event.is_set():\n",
    "        data, _ = sock.recvfrom(BUFFER_SIZE)\n",
    "        image_data.extend(data)\n",
    "\n",
    "        if data.endswith(END_MARKER):\n",
    "            try:\n",
    "                image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "                image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "                queue_image.put(image)\n",
    "\n",
    "                if queue_image.qsize() >= MAX_QUEUE_SIZE:\n",
    "                    queue_image.get()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert image: {str(e)}\")\n",
    "\n",
    "            image_data = bytearray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define constants for different directions and commands\n",
    "STRAIGHT = 1      # Move straight\n",
    "SLOW = 2          # Slow down\n",
    "TURNLEFT = 5      # Turn left\n",
    "TURNRIGHT = 6     # Turn right\n",
    "FORWARD = 7       # Move forward\n",
    "BACKWARD = 8      # Move backward\n",
    "STANDBY = 9       # Standby mode\n",
    "STOP = 10         # Stop\n",
    "\n",
    "\n",
    "def handle_sign_command(sign_list):\n",
    "    \"\"\"\n",
    "    Determine the command to send based on the detected traffic signs.\n",
    "\n",
    "    Parameters:\n",
    "    - sign_list: A list of detected traffic signs, which may include colors and directions.\n",
    "\n",
    "    Returns:\n",
    "    - cmd: The command corresponding to the detected traffic signs.\n",
    "    \"\"\"\n",
    "    # If the traffic light is red, stop the vehicle\n",
    "    if 'red' in sign_list:\n",
    "        cmd = STOP\n",
    "        return cmd\n",
    "\n",
    "    # If the sign indicates to stop, set the vehicle to standby mode\n",
    "    if 'stop' in sign_list:\n",
    "        cmd = STANDBY\n",
    "        return cmd\n",
    "\n",
    "    # If the traffic light is yellow, slow down the vehicle\n",
    "    if 'yellow' in sign_list:\n",
    "        cmd = SLOW\n",
    "        return cmd\n",
    "\n",
    "    # If the sign indicates to turn left, set the turn left command\n",
    "    if 'left' in sign_list:\n",
    "        cmd = TURNLEFT\n",
    "        return cmd\n",
    "\n",
    "    # If the sign indicates to turn right, set the turn right command\n",
    "    if 'right' in sign_list:\n",
    "        cmd = TURNRIGHT\n",
    "        return cmd\n",
    "\n",
    "    # If the traffic light is green or indicates to go straight, move forward\n",
    "    if 'straight' in sign_list or 'green' in sign_list:\n",
    "        cmd = FORWARD\n",
    "        return cmd\n",
    "\n",
    "    # Default command if no specific sign is detected\n",
    "    else:\n",
    "        cmd = STRAIGHT\n",
    "        return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_traffic_sign():\n",
    "    \"\"\"Detect traffic signs and send commands based on detection results.\"\"\"\n",
    "    global stop_event\n",
    "\n",
    "    # Create a socket for sending UDP messages\n",
    "    sock = create_udp_socket_send()\n",
    "\n",
    "    # Initialize variables for FPS calculation\n",
    "    prev_time = time.time()\n",
    "    frame_count = 0\n",
    "    fps = 0\n",
    "    prev_cmd = STOP\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        sign_list = set()\n",
    "        try:\n",
    "            # Get an image from the queue\n",
    "            image = queue_image.get()\n",
    "\n",
    "            # Update frame count and calculate FPS\n",
    "            frame_count += 1\n",
    "            tmp_time = time.time() - prev_time\n",
    "            if tmp_time >= 1:\n",
    "                fps = frame_count / tmp_time\n",
    "                prev_time = time.time()\n",
    "                frame_count = 0\n",
    "\n",
    "            # Score the frame using the traffic signs detector\n",
    "            results = traffic_signs_detector.score_frame(image)\n",
    "\n",
    "            # Get bounding boxes and class labels for detected objects\n",
    "            detections = traffic_signs_detector.plot_boxes(\n",
    "                results, height=image.shape[0], width=image.shape[1], confidence=0.5)\n",
    "\n",
    "            # Update object tracks\n",
    "            tracks = object_tracker.update_tracks(detections, frame=image)\n",
    "\n",
    "            for track in tracks:\n",
    "                if not track.is_confirmed():\n",
    "                    continue\n",
    "\n",
    "                # Get tracking information\n",
    "                track_id = track.track_id\n",
    "                bbox_x1, bbox_y1, bbox_x2, bbox_y2 = map(int, track.to_ltrb())\n",
    "                class_label = track.det_class\n",
    "                conf = track.det_conf if track.det_conf is not None else 0.0\n",
    "\n",
    "                # If the object is at the edge of the frame, mark it as a detected sign\n",
    "                if bbox_y2 >= image.shape[0] - 10 or bbox_x1 < 10 or bbox_x2 > image.shape[1] - 10:\n",
    "                    sign_list.add(class_label)\n",
    "\n",
    "                    # Draw bounding box with green color for detected signs\n",
    "                    cv2.rectangle(image, (bbox_x1, bbox_y1),\n",
    "                                  (bbox_x2, bbox_y2), (0, 255, 0), 2)\n",
    "                else:\n",
    "                    # Draw bounding box with red color for normal tracking\n",
    "                    cv2.rectangle(image, (bbox_x1, bbox_y1),\n",
    "                                  (bbox_x2, bbox_y2), (0, 0, 255), 2)\n",
    "\n",
    "                # Annotate the image with tracking information\n",
    "                cv2.putText(image, \"ID: \" + str(track_id), (bbox_x1, bbox_y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(image, f'Class: {class_label}', (bbox_x1, bbox_y1 - 45),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(image, f'Conf: {conf:.2f}', (bbox_x1, bbox_y1 - 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            # Determine the command based on detected signs\n",
    "            cmd = handle_sign_command(sign_list)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle errors in image processing\n",
    "            print(\"Error processing image:\", e)\n",
    "            cmd = STOP\n",
    "\n",
    "        # Send the command via UDP if it has changed\n",
    "        if cmd != prev_cmd:\n",
    "            try:\n",
    "                data_cmd = f\"{cmd}\"\n",
    "                data_bytes_cmd = data_cmd.encode()\n",
    "                sock.sendto(data_bytes_cmd, (ip_lap_lead, port_lap_lead))\n",
    "                print(f\"Sent UDP message: '{cmd}' to {\n",
    "                      ip_lap_lead}:{port_lap_lead}\")\n",
    "                prev_cmd = cmd\n",
    "            except Exception as e:\n",
    "                print(f\"Error sending UDP message: {e}\")\n",
    "\n",
    "        # Display the FPS on the image\n",
    "        cv2.putText(image, f'FPS: {int(fps)}', (20, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the processed image\n",
    "        cv2.imshow('image', image)\n",
    "\n",
    "        # Exit the loop if the 'Esc' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            stop_event.set()\n",
    "            break\n",
    "\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Traffic sign detection model by using laptop camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "stop_event = threading.Event()\n",
    "\n",
    "threads = [\n",
    "    threading.Thread(target=get_image_from_leader,\n",
    "                     name='GetSendImageThread'),\n",
    "    threading.Thread(target=detect_traffic_sign,\n",
    "                     name='ProcessImageThread'),\n",
    "]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Implement Traffic sign detection model to control AutoCar kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "stop_event = threading.Event()\n",
    "\n",
    "threads = [\n",
    "    threading.Thread(target=get_image_from_cam,\n",
    "                     name='GetSendImageThread'),\n",
    "    threading.Thread(target=detect_traffic_sign,\n",
    "                     name='ProcessImageThread'),\n",
    "]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
