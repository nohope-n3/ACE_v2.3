{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1\n",
    "\n",
    "Welcome to Exercise 3.1. In this exercise, we will learn how to use the DeepSORT algorithm with the YOLOv5 model to track objects in video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Below are detailed instructions to help you understand the process of applying DeepSORT and YOLO to video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Detection Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force reload: avoid parameter conflicts when loading new models\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path= r\"[...]\\best.pt\", force_reload = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSORT\n",
    "object_tracker = DeepSort(max_age=3,\n",
    "                          n_init=2,\n",
    "                          nms_max_overlap=1.0,\n",
    "                          max_cosine_distance=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input processing functions for the DeepSORT algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_frame(frame):\n",
    "\tresults = model(frame)\n",
    "\tlabels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "\treturn labels, cord\n",
    "\n",
    "classes = model.names\n",
    "\n",
    "def class_to_label(x):\n",
    "\treturn classes[int(x)]\n",
    "\n",
    "def plot_boxes(results, height, width, confidence=0.3):\n",
    "\tlabels, cord = results\n",
    "\tdetections = []\n",
    "\n",
    "\tfor i in range(len(labels)):\n",
    "\t\trow = cord[i]\n",
    "\t\tif row[4] >= confidence:\n",
    "\t\t\tx1, y1, x2, y2 = int(\n",
    "\t\t\t\trow[0]*width), int(row[1]*height), int(row[2]*width), int(row[3]*height)\n",
    "\n",
    "\t\t\tconf = float(row[4].item())\n",
    "\t\t\tclass_label = class_to_label(labels[i])\n",
    "\t\t\t# print(feature)\n",
    "\t\t\tdetections.append(\n",
    "\t\t\t\t([x1, y1, int(x2-x1), int(y2-y1)], conf, class_label))\n",
    "\n",
    "\treturn detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply DeepSORT and YOLO to Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = [...] # Path to input video\n",
    "output_video_path = [...]  # Path to input video\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get video parameters\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create video writer to save output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "while cap.isOpened():\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Predict objects in the frame\n",
    "\n",
    "\tresults = score_frame(frame)\n",
    "\tdetections = plot_boxes(results=results, height=frame.shape[0], width=frame.shape[1], confidence=0.5)\n",
    "\ttracks = object_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "\t# Draw frames and track IDs onto the video\n",
    "\tfor track in tracks:\n",
    "\t\tbbox = track.to_tlbr()  # Bounding box as (x1, y1, x2, y2)\n",
    "\t\ttrack_id = track.track_id  # ID tracking\n",
    "\t\tx1, y1, x2, y2 = map(int, bbox)\n",
    "\t\tcv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\t\tcv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\t# Record frames to output video\n",
    "\tout.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
