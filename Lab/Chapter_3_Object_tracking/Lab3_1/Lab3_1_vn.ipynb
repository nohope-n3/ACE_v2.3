{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài thực hành số 3.1\n",
    "\n",
    "Chào mừng đến với bài thực hành số 3.1. Trong bài thực hành này ta sẽ học cách sử dụng thuật toán DeepSORT với model YOLOv5 để tracking vật thể trong video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng quan\n",
    "\n",
    "DeepSORT (Simple Online and Realtime Tracking) là một thuật toán theo dõi đối tượng trực tuyến mạnh mẽ, được phát triển dựa trên thuật toán SORT. DeepSORT cải tiến SORT bằng cách kết hợp các tính năng học sâu để theo dõi các đối tượng trong video với độ chính xác cao hơn, đặc biệt là trong các tình huống phức tạp như có nhiều đối tượng và các đối tượng xuất hiện hoặc biến mất khỏi khung hình.\n",
    "\n",
    "DeepSORT được giới thiệu lần đầu tiên vào năm 2017 trong bài báo có tên \"Simple Online and Realtime Tracking with a Deep Association Metric\" bởi các tác giả là Wojke, Bewley, và Paulus. Sự cải tiến chính của DeepSORT so với SORT là việc sử dụng mạng nơ-ron sâu để tạo ra các vector đặc trưng giúp phân biệt giữa các đối tượng khác nhau, ngay cả khi có sự thay đổi về vị trí hay hình dạng.\n",
    "\n",
    "Trong lĩnh vực xe tự hành (autonomous vehicles), DeepSORT có thể được sử dụng trong các hệ thống theo dõi đối tượng để nhận diện và theo dõi các phương tiện khác, người đi bộ, và các chướng ngại vật trên đường. Khả năng theo dõi liên tục và chính xác của DeepSORT giúp xe tự hành có thể đưa ra quyết định di chuyển an toàn hơn, tránh va chạm và đảm bảo lộ trình di chuyển được duy trì một cách hiệu quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mục tiêu học tập\n",
    "Sau khi hoàn thành bài thực hành này, học viên sẽ học được các kiến thức:\n",
    "  - DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Những kiến thức liên quan\n",
    "  - Python\n",
    "  - DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nền tảng cần thiết\n",
    "Để thực hiện được bài thực hành, bạn sẽ cần có những kiến thức sau:\n",
    "  - Kĩ năng lập trình cơ bản với Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài toán\n",
    "**Mục tiêu**: Áp dụng DeepSORT với model YOLO để theo dõi vật thể trong video\n",
    "\n",
    "**Yêu cầu**\n",
    "- Input: video\n",
    "- Output: video có các ô theo dõi vật thể"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hướng dẫn\n",
    "\n",
    "Dưới đây là hướng dẫn chi tiết các bước để bạn có thể hiểu được quá trình áp dụng DeepSORT và YOLO lên video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cài đặt thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tải model nhận diện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Việc các vật thể nào được theo dõi sẽ phụ thuộc vào việc model có khả năng nhận diện được hay không. Ở các bài lab chương 2, chúng tôi đã hướng dẫn bạn cách để có thể train được model hiệu quả cho từng mục đích. Hãy tải lại weights model bạn thấy phù hợp để hoàn thành bài tập sau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài tập 1: Hoàn thành đoạn code sau bằng việc điền vào [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force reload: tránh xung đột tham số khi load model mới\n",
    "model = torch.hub.load('ultralytics/yolov5', [...], path= [...], force_reload = [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khởi tạo DeepSORT\n",
    "Ý nghĩa các tham số được sử dụng trong hàm `DeepSort`:\n",
    "\n",
    "1. **`max_age`**:\n",
    "   - **Ý nghĩa**: Xác định số khung hình tối đa mà một đối tượng có thể bị mất dấu trước khi bị loại khỏi bộ theo dõi. \n",
    "   - **Giải thích**: Nếu một đối tượng không được phát hiện trong `max_age` khung hình liên tiếp, nó sẽ bị coi là mất dấu và bị loại bỏ.\n",
    "\n",
    "2. **`n_init`**:\n",
    "   - **Ý nghĩa**: Xác định số lượng khung hình ban đầu mà một đối tượng cần phải được phát hiện liên tiếp trước khi nó được coi là một đối tượng theo dõi hợp lệ.\n",
    "   - **Giải thích**: Một đối tượng mới phải xuất hiện trong ít nhất `n_init` khung hình trước khi nó được xác nhận là một đối tượng hợp lệ và bắt đầu được theo dõi.\n",
    "\n",
    "3. **`nms_max_overlap`**:\n",
    "   - **Ý nghĩa**: Tham số này liên quan đến Non-Maximum Suppression (NMS), một thuật toán được sử dụng để loại bỏ các khung giới hạn (bounding box) chồng lấp nhau với các đối tượng khác nhau.\n",
    "   - **Giải thích**: `nms_max_overlap` xác định mức độ chồng lấp tối đa giữa các khung giới hạn mà vẫn được coi là hai đối tượng riêng biệt. Giá trị mặc định là `1.0`, có nghĩa là các khung giới hạn có thể chồng lấp hoàn toàn mà không bị loại bỏ.\n",
    "\n",
    "4. **`max_cosine_distance`**:\n",
    "   - **Ý nghĩa**: Xác định khoảng cách cosine tối đa giữa các đặc trưng của các đối tượng (ví dụ như embedding vectors) để có thể coi chúng là cùng một đối tượng.\n",
    "   - **Giải thích**: Khoảng cách cosine là một thước đo sự tương tự giữa hai vectơ. Giá trị `max_cosine_distance` thấp cho thấy yêu cầu cao về sự tương tự giữa hai đối tượng để coi chúng là cùng một đối tượng. Giá trị mặc định thường là `0.3`, có nghĩa là chỉ các đối tượng có sự tương đồng cao mới được ghép nối.\n",
    "\n",
    "Các tham số này có thể được điều chỉnh dựa trên dữ liệu và bài toán cụ thể mà bạn đang làm việc để tối ưu hóa hiệu suất của hệ thống theo dõi đối tượng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài tập 2: Hoàn thành đoạn code sau bằng cách điền vào [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo DeepSORT\n",
    "object_tracker = DeepSort(max_age=[...],\n",
    "                          n_init=[...],\n",
    "                          nms_max_overlap=[...],\n",
    "                          max_cosine_distance=[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các hàm xử lý đầu vào cho thuật toán DeepSORT\n",
    "\n",
    "Hàm `score_frame(frame)`: Hàm này nhận một khung hình (frame) làm đầu vào và trả về nhãn và tọa độ của các đối tượng được phát hiện trong khung hình.\n",
    "\n",
    "Biến `classes`: là một danh sách hoặc từ điển chứa các tên lớp của mô hình YOLO, với chỉ số của lớp tương ứng với tên của đối tượng.\n",
    "  \n",
    "Hàm `class_to_label(x)`: Hàm này nhận vào chỉ số của lớp (x) và trả về tên lớp tương ứng.\n",
    "\n",
    "Hàm `plot_boxes(results, height, width, confidence=0.3)`: Hàm này xử lý kết quả dự đoán của mô hình, lọc các đối tượng dựa trên độ tin cậy (confidence) và chuẩn bị thông tin về các hộp giới hạn (bounding boxes) để vẽ lên khung hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài tập 3: Hoàn thành đoạn code sau bằng cách điền vào [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm để nhận khung hình và trả về các nhãn lớp và tọa độ của đối tượng\n",
    "def score_frame(frame):\n",
    "\tresults = [...]\n",
    "\tlabels, cord = [...]\n",
    "\treturn [...]\n",
    "\n",
    "classes = model.names\n",
    "\n",
    "# Dựa trên chỉ số lớp, trả về tên lớp tương ứng\n",
    "def class_to_label(x):\n",
    "\treturn [...]\n",
    "\n",
    "# Hoàn thiện hàm để lọc và chuẩn bị thông tin về các đối tượng phát hiện được\n",
    "def plot_boxes(results, height, width, confidence=0.3):\n",
    "\tlabels, cord = [...]\n",
    "\tdetections = []\n",
    "\n",
    "\tfor i in range(len(labels)):\n",
    "\t\trow = cord[i]\n",
    "\t\tif [...]:\n",
    "\t\t\tx1, y1, x2, y2 = [...]\n",
    "\t\t\tconf = [...]\n",
    "\t\t\tclass_label = [...]\n",
    "\t\t\t# print(feature)\n",
    "\t\t\tdetections.append(\n",
    "\t\t\t\t([x1, y1, int(x2-x1), int(y2-y1)], conf, class_label))\n",
    "\n",
    "\treturn detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Áp dụng DeepSORT và YOLO vào video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài tập 4: Hoàn thành code sau bằng việc điền vào [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = [...] # Đường dẫn đến video đầu vào\n",
    "output_video_path = [...]  # Đường dẫn đến video đầu ra\n",
    "\n",
    "# Mở video đầu vào\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Lấy thông số của video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Tạo video writer để lưu video đầu ra\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec cho định dạng MP4\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "while cap.isOpened():\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Dự đoán các đối tượng trong khung hình\n",
    "\n",
    "\tresults = score_frame([...])\n",
    "\tdetections = plot_boxes([...])\n",
    "\ttracks = object_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "\t# Vẽ các khung hình và các ID theo dõi lên video\n",
    "\tfor track in tracks:\n",
    "\t\tbbox = track.to_tlbr()  # Bounding box dưới dạng (x1, y1, x2, y2)\n",
    "\t\ttrack_id = track.track_id  # ID theo dõi\n",
    "\t\tx1, y1, x2, y2 = map(int, bbox)\n",
    "\t\tcv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\t\tcv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\t# Ghi khung hình vào video đầu ra\n",
    "\tout.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
