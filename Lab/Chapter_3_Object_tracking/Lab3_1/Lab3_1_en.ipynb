{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1\n",
    "\n",
    "Welcome to Exercise 3.1. In this exercise, we will learn how to use the DeepSORT algorithm with the YOLOv5 model to track objects in video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "DeepSORT (Simple Online and Realtime Tracking) is a powerful online object tracking algorithm, developed as an enhancement to the SORT algorithm. DeepSORT improves SORT by integrating deep learning features to track objects in video with higher accuracy, especially in complex situations such as multiple objects and objects appearing or disappearing from the frame.\n",
    "\n",
    "DeepSORT was first introduced in 2017 in the paper titled \"Simple Online and Realtime Tracking with a Deep Association Metric\" by Wojke, Bewley, and Paulus. The main improvement of DeepSORT over SORT is the use of a deep neural network to generate feature vectors that help distinguish between different objects, even when there are changes in position or shape.\n",
    "\n",
    "In the field of autonomous vehicles, DeepSORT can be used in object tracking systems to identify and track other vehicles, pedestrians, and obstacles on the road. The continuous and accurate tracking capability of DeepSORT allows autonomous vehicles to make safer movement decisions, avoid collisions, and maintain an efficient travel path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "After completing this exercise, learners will gain knowledge of:\n",
    "- DeepSORT\n",
    "\n",
    "## Related Knowledge\n",
    "- Python\n",
    "- DeepSORT\n",
    "\n",
    "## Prerequisites\n",
    "To complete this exercise, you will need the following knowledge:\n",
    "- Basic programming skills in Python\n",
    "\n",
    "## Problem Statement\n",
    "**Objective**: Apply DeepSORT with YOLO model to track objects in a video.\n",
    "\n",
    "**Requirements**:\n",
    "- Input: video\n",
    "- Output: video with object tracking boxes\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Below are detailed instructions to help you understand the process of applying DeepSORT and YOLO to video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Detection Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects that will be tracked depend on whether the model can detect them. In the labs of Chapter 2, we have guided you on how to effectively train models for different purposes. Please reload the weights of the model you find suitable to complete the following task.\n",
    "\n",
    "**Exercise 1**: Complete the following code by filling in [...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force reload: avoid parameter conflicts when loading new models\n",
    "model = torch.hub.load('ultralytics/yolov5', [...], path= [...], force_reload = [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DeepSORT\n",
    "The meaning of the parameters used in the `DeepSort` function:\n",
    "\n",
    "1. **`max_age`**:\n",
    "   - **Meaning**: Specifies the maximum number of frames that an object can be lost before it is removed from the tracker. \n",
    "   - **Explanation**: If an object is not detected for `max_age` consecutive frames, it will be considered lost and removed.\n",
    "\n",
    "2. **`n_init`**:\n",
    "   - **Meaning**: Specifies the number of initial frames in which an object must be detected consecutively before it is considered a valid tracked object.\n",
    "   - **Explanation**: A new object must appear in at least `n_init` frames before it is confirmed as a valid object and starts being tracked.\n",
    "\n",
    "3. **`nms_max_overlap`**:\n",
    "   - **Meaning**: This parameter relates to Non-Maximum Suppression (NMS), an algorithm used to eliminate overlapping bounding boxes with different objects.\n",
    "   - **Explanation**: `nms_max_overlap` defines the maximum overlap between bounding boxes that is still considered as two separate objects. The default value is `1.0`, meaning that bounding boxes can overlap completely without being removed.\n",
    "\n",
    "4. **`max_cosine_distance`**:\n",
    "   - **Meaning**: Specifies the maximum cosine distance between the features of objects (e.g., embedding vectors) to be considered the same object.\n",
    "   - **Explanation**: Cosine distance is a measure of similarity between two vectors. A low `max_cosine_distance` value indicates a high requirement for similarity between two objects to be matched. The default value is typically `0.3`, meaning that only highly similar objects will be matched.\n",
    "\n",
    "These parameters can be adjusted based on the data and specific problem you are working on to optimize the performance of the object tracking system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Complete the following code by filling in [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSORT\n",
    "object_tracker = DeepSort(max_age=[...],\n",
    "                          n_init=[...],\n",
    "                          nms_max_overlap=[...],\n",
    "                          max_cosine_distance=[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input processing functions for the DeepSORT algorithm\n",
    "\n",
    "Function `score_frame(frame)`: This function takes a frame as input and returns the labels and coordinates of the objects detected in the frame.\n",
    "\n",
    "Variable `classes`: is a list or dictionary containing the class names of the YOLO model, with the class index corresponding to the object name.\n",
    "  \n",
    "Function `class_to_label(x)`: This function takes the class index (x) and returns the corresponding class name.\n",
    "\n",
    "Function `plot_boxes(results, height, width, confidence=0.3)`: This function processes the model's prediction results, filters objects based on confidence, and prepares bounding box information to be drawn on the frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Complete the following code by filling in [...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to receive the frame and return the object's layer labels and coordinates\n",
    "def score_frame(frame):\n",
    "\tresults = [...]\n",
    "\tlabels, cord = [...]\n",
    "\treturn [...]\n",
    "\n",
    "classes = model.names\n",
    "\n",
    "# Based on the class index, returns the corresponding class name\n",
    "def class_to_label(x):\n",
    "\treturn [...]\n",
    "\n",
    "# Complete the function to filter and prepare information about detected objects\n",
    "def plot_boxes(results, height, width, confidence=0.3):\n",
    "\tlabels, cord = [...]\n",
    "\tdetections = []\n",
    "\n",
    "\tfor i in range(len(labels)):\n",
    "\t\trow = cord[i]\n",
    "\t\tif [...]:\n",
    "\t\t\tx1, y1, x2, y2 = [...]\n",
    "\t\t\tconf = [...]\n",
    "\t\t\tclass_label = [...]\n",
    "\t\t\t# print(feature)\n",
    "\t\t\tdetections.append(\n",
    "\t\t\t\t([x1, y1, int(x2-x1), int(y2-y1)], conf, class_label))\n",
    "\n",
    "\treturn detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply DeepSORT and YOLO to Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Complete the following code by filling in [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = [...] # Path to input video\n",
    "output_video_path = [...]  # Path to input video\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get video parameters\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create video writer to save output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "while cap.isOpened():\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Predict objects in the frame\n",
    "\n",
    "\tresults = score_frame([...])\n",
    "\tdetections = plot_boxes([...])\n",
    "\ttracks = object_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "\t# Draw frames and track IDs onto the video\n",
    "\tfor track in tracks:\n",
    "\t\tbbox = track.to_tlbr()  # Bounding box as (x1, y1, x2, y2)\n",
    "\t\ttrack_id = track.track_id  # ID tracking\n",
    "\t\tx1, y1, x2, y2 = map(int, bbox)\n",
    "\t\tcv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\t\tcv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\t# Record frames to output video\n",
    "\tout.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
