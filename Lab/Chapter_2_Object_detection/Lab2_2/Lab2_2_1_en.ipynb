{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 2.2.1\n",
    "\n",
    "Welcome to Lab 2.2.1, which is a continuation of Lab 2.2 on Colab. In this lab, we will guide you on how to apply the model you just trained to detect objects in videos or via your laptop's camera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Below is a detailed guide to help you understand how to apply YOLO to videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.hub.load` function in PyTorch is a powerful tool for loading models or modules from repositories shared on GitHub. It allows you to easily use pre-trained models or other modules without having to build them from scratch.\n",
    "\n",
    "Structure of the `torch.hub.load` function:\n",
    "\n",
    "```python\n",
    "model = torch.hub.load(repo_or_dir, model, *args, **kwargs)\n",
    "```\n",
    "\n",
    "Parameters of the function:\n",
    "\n",
    "1. **`repo_or_dir`**: \n",
    "   - This parameter specifies the GitHub repository containing the model you want to load. It is provided as a string, e.g., `'ultralytics/yolov5'`.\n",
    "   - You can also specify a local path to the directory containing the model if the model has already been downloaded.\n",
    "\n",
    "2. **`model`**: \n",
    "   - The name of the model or module you want to load from the repository. For example, `'custom'` for a custom model or `'yolov5s'` for a pre-trained YOLOv5s model.\n",
    "   - Depending on the repository, this name may represent a specific model or a set of predefined models.\n",
    "\n",
    "3. **`path`**: \n",
    "   - The path to the `.pt` file containing the trained model weights. This parameter is usually used when you want to load a pre-trained model that you have built or customized.\n",
    "\n",
    "4. **`force_reload`**: \n",
    "   - This parameter is a boolean value (`True` or `False`).\n",
    "   - When `force_reload=True`, the function will reload the model from the repository or the specified path even if the model has already been loaded before. This is useful when you want to avoid conflicts or ensure that you are using the latest version of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Complete the following code by filling in the [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force reload: avoid parameter conflicts when loading new models\n",
    "model = torch.hub.load('ultralytics/yolov5', [...], path= [...], force_reload = [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Images\n",
    "\n",
    "Before working with videos, we will first apply the model to a few images to observe its effectiveness.\n",
    "\n",
    "**`model(img_path)`**: This command uses the YOLOv5 model that was previously loaded to detect objects in the image.\n",
    "- **`img_path`** is the path to the image you want to run object detection on.\n",
    "- When you call the model with an image path like this, the model processes the image and returns the detection results as a `results` object.\n",
    "\n",
    "**`test_detect.show()`**: This method displays the image with bounding boxes drawn around the detected objects.\n",
    "- The detected objects are shown with information such as class labels and the confidence level of each object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "\n",
    "cv2.imshow(\"test\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detect = model(img_path)\n",
    "test_detect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Complete the exercise by filling in the [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = [...]  # Path to input video\n",
    "output_video_path = [...]  # Path to output video\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get video parameters\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create video writer to save output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frames from BGR to RGB\n",
    "    rgb_frame = [...]\n",
    "\n",
    "    # Predict with YOLOv5\n",
    "    results = model([...])\n",
    "\n",
    "    # Get the prediction result as a DataFrame\n",
    "    df = results.pandas().xyxy[0]\n",
    "\n",
    "    # Draw prediction boxes on the canvas\n",
    "    for _, row in df.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "        label = row['name']\n",
    "        score = row['confidence']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        # Draw class labels and confidence levels\n",
    "        cv2.putText(frame, f'{label} {score:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Record frames to output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Free up resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
